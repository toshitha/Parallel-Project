---
title: "Sparse Matrix Multiplication"
---

by Anisha Padwekar and Toshitha Jagadeesh

## Summary
We will be implementing Sparse Matrix Multiplication in parallel for this project. Our goal is to find the most efficient way to do this computation in parallel.

## Background
Sparse matrices are matrices that have many zero elements in the matrix. Sparse matrices are used in different areas of study from combinatoricsto network theory. When using these matrices to analyze

## The Challenge
This problem is challenging because we will be attempting to speed up the process of multiplying matrices together. We have some ideas from different papers and our prior knowledge in ways to make the multiplication more efficient. However, we will have to test out the different methods such as blocking, etc. Likewise, sparse matrices are different from regular dense matrix multiplication due to the higher presence of zeros. Due to this factor, the matrices themselves can be represented slightly differently in order to make the multiplication more efficient (since 0 values would not need to multiplied). We foresee that finding the best way to represent the matrices will be difficult.

## Resources
We will be using the GHC machines to complete our project. As of now, our code base will be starting from scratch. However, we will use a couple papers as reference for ways to optimize the multiplication. 
https://arxiv.org/ftp/arxiv/papers/1006/1006.2183.pdf


## Goals and Deliverables

##### *Plan to Achieve*
We plan to implement the different algorithms in the paper on the gates machine and optimize to the 4 core processor. We will also create a graph that shows the speed of these algorithms in comparison with each other on a pair of matrices we will consider to be the benchmark.

##### *Hope To Achieve*
We hope to be able to create a disitributed version of sparse matrix multiplication similar to what we did in Assignment 3. We hope to compare this with our regular version and see which ones work better.


## Platform Choice
We decided to work on the GHC machines because we plan to use OpenMP to parallelize our code. However, we will be just doing it on a single machine. Therefore, the GHC machines are the most accessible/easiest to use. If we finish this part and move on to our HOPE TO ACHIEVE goals, then we will also need the use of the Xeon Phi machines since we will be distributing the work over multiple machines.

## Schedule
*4/17 - 4/23:* Understand the different optimizations and implement a serial version of sparse matrix multiplication.

*4/24 - 4/30:* Find data sets (matrices) that we can use. Begin to implement the different optimizations on the GHC machines.

*5/1 - 5/7:* Finish all code and test on different matrices.

*5/8 - 5/10:* Finalize all the code and finish testing. Run on our benchmark matrices and create the final presentation.
