---
title: "Checkpoint"
---

We were able to understand the algorithms and optimizations presented in the paper that we wanted to follow. With our deeper understanding of the material, we have started brainstorming ways to create the parallel versions of the methods used for sparse matrix multiplication. However, understanding the methods took a little longer than expected so we are a little behind in testing the serial version of the code. 

We were also able to find the data set that we will select matrices from found here (https://www.cise.ufl.edu/research/sparse/matrices/list_by_id.html). In finding this data set, we mitigated one of the challenges we originially had in terms of figuring out how to represent our sparse matrix initially. 

Other than our small set back, we are still on track with or goals and deliverables. We plan to finish implementing the parallel optimizations of sparse matrix multiplication in the upcoming week. It seems to be somewhat unlikely that we will get to creating distributed sparse matrix multiplication but we will continue to strive for this optimization. Likewise, at the parallelism competition, we plan to display a graph of the runtime of the different methods for sparse matrix multiplication on a pair of baseline matrices.

The biggest issue we are concerned with are running into bugs/implementation problems. Since our paper is purely theoretical, we have to create the algorithms from scratch which includes representing the graph and finding the most efficient way to parallelize the code.